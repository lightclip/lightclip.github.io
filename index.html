<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Cfinbench">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LightCLIP: Enhancing Light CLIP with Hierarchical Semantic Alignment and Fusion</title>
  <link rel="icon" type="image/x-icon" href="static/images/cfinbench.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LightCLIP: Enhancing Light CLIP with Hierarchical Semantic Alignment and Fusion</h1>
            <div class="is-size-5 publication-authors">
              <div class="publication-authors">
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=1eOYln4AAAAJ&hl=en" target="_blank">Ying Nie</a><sup>∗</sup>
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=3Q7wBlMAAAAJ&hl=en" target="_blank">Wei He</a><sup>∗</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=vThoBVcAAAAJ&hl=en" target="_blank">Kai Han</a><sup>†</sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=TkSZQ6gAAAAJ&hl=zh-CN" target="_blank">Yehui Tang</a><sup></sup>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=isizOkYAAAAJ&hl=en" target="_blank">Yunhe Wang</a><sup>†</sup>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  Huawei Noah’s Ark Lab<br>
                </span>
                <span class="eql-cntrb">
                  <small><br>
                    <sup>∗</sup>Equal Contribution<br>
                    <sup>†</sup>Corresponding Author
                  </small>
                </span>
              </div>
            </div> 


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/2407.02301" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2407.02301" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> --> 

                <!-- dataset link >
                <span class="link-block">
                  <a href="https://drive.google.com/file/d/1VjlJMsThbQwIcUaib8D5hNKA3gQoEZ_B/view" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-google-drive"></i>
                    </span>
                    <span>Dataset</span>
                  </a>

                </span>
                <!-- Github link >
                <span class="link-block">
                  <a href="https://github.com/yanbinwei/CFinBench-Eval/tree/master" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div-->


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision-language pre-training like CLIP has shown promising performance on various downstream tasks such as zero-shot image classification and image-text retrieval. 
              Most of the existing CLIP-alike works usually adopt relatively large encoders like ResNet-50 and ViT-B/32, while the light counterparts are rarely discussed. 
              In this paper, we propose a method of hierarchical semantic alignment and fusion for training light CLIP models. 
              Firstly, to mitigate the issue that some imagetext pairs are not strictly one-to-one correspondence, we improve the conventional objective of instance-level alignment 
              by softening the label of negative samples progressively. Secondly, a novel relaxed bipartite matching based objective of token-level alignment is introduced 
              for a finer-grained alignment between image patches and textual words. Moreover, the objective of masked language modeling (MLM) is leveraged for maximizing 
              the potential of text encoder. In practice, an auxiliary fusion module injecting unmasked image embedding into masked text embedding 
              at various network stages is proposed for enhancing the MLM. Extensive experiments show that without introducing any additional computational cost 
              during inference, the proposed method achieves a higher performance on multiple downstream tasks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


<!-- Single image section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="has-text-centered">
        <!-- TODO: Replace with your research result image -->
        <img src="static/images/motivation.png" alt="Research result visualization" loading="lazy" style="max-width: 100%; height: auto;"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Motivations(a) Some image-text pairs in YFCC15M-V2 dataset are many-to-many correspondence, indicating
the one-hot label for instance-level alignment is sub-optimal; (b) Adopting only the objective of instance-level alignment usually
results in the match failure between image patches and textual words; (c) Under the same text encoder (8-layer Transformer), the
ImageNet zero-shot accuracy improves accordingly when Swin-Nano, Swin-Tiny, Swin-Small and Swin-Base are adopted
as the image encoder, respectively. However, under the same image encoder (MobileNet-V2), the accuracy does not improve
significantly when 4-layer, 8-layer, 12-layer and 16-layer Transformer, are adopted as the text encoder, respectively.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End single image section -->


<!-- Single image section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="has-text-centered">
        <!-- TODO: Replace with your research result image -->
        <img src="static/images/framework.png" alt="Research result visualization" loading="lazy" style="max-width: 100%; height: auto;"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          (a) Image and text encoder are each divided into multiple stages, the complete
objective includes contrastive instance-level alignment, relaxed token-level alignment, and the enhanced MLM through imageto-text embedding fusion; (b) The embedding fusion module of unmasked image to masked text; (c) The progressive softened
label utilized in contrastive instance-level alignment; (d) The objective of relaxed token-level alignment, where Sim(·, ·) is
cosine similarity, Bi-Match is Bipartite-Matching algorithm for achieving the one-to-one match between image patches and
textual words with the lowest matching cost. ”Relaxed” means only the paired image-text is involved for alignment.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End single image section -->
                

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/cam-resnet-18.png" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Visualization on the match between image patches and textual words of ResNet-18.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/cam-mv2.png" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Visualization on the match between image patches and textual words of MobileNet-V2.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/cam-swinn.png" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Visualization on the match between image patches and textual words of Swin-Nano.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->
                

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
